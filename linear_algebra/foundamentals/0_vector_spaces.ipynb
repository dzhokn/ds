{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "A vector space is a non-empty set $V$ of objects, called vectors, on which are defined two operations, called **addition** and **multiplication** by scalars (real numbers).\n",
    "\n",
    "We can think of a vector space in general, as a collection of objects that behave as vectors do in $\\R^n$. The objects of such a set\n",
    "are called vectors.\n",
    "\n",
    "Vector spaces are subject to next ten axioms for all $u, v$ and $w$ in $V$ and for all scalars $c$ and $d$.\n",
    "1. $u + v \\in V$ \n",
    "2. $u + v = v + u$\n",
    "3. $(u + v) + w = u + (v + w)$\n",
    "4. There is a vector $0 \\in V$ such that $u + 0 = u$\n",
    "5. For $\\forall u$  exists ${-u}$ such that $u + (−u) = 0$\n",
    "6. $cu \\in V$\n",
    "7. $c(u + v) =cu+cv$\n",
    "8. $(c + d)u = cu + du$\n",
    "9. $(cd)u = c(du)$\n",
    "10. There is a vector $1 \\in V$ such that $1u = u$\n",
    "\n",
    "## Vector Space Examples\n",
    "### Field (1-D)\n",
    "A field $F$ is basically 1-dimensional vector space over itself.\n",
    "\n",
    "Vector addition is just field addition, and scalar multiplication is just field multiplication. This property can be used to prove that a field is a vector space. Any non-zero element of $F$ serves as a **basis** (because we can represent all other elements in $F$ by it).\n",
    "\n",
    "### Coordinate Space (n-D)\n",
    "Every coordinate space $\\R^n$ is an $n$-dimensional vector space over the field $\\R$. An element of $\\R^n$ is written:\n",
    "$$u = (u_1, u_2, ..., u_n)$$\n",
    "\n",
    "The vector space $\\R^n$ has a standard basis:\n",
    "$$ e_1 = (1, 0, ..., 0) $$\n",
    "$$ e_2 = (0, 1, ..., 0) $$\n",
    "$$ \\vdots $$\n",
    "$$ e_n = (0, 0, ..., 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Origin\n",
    "Any vector space must have a $0$-vector $(0,0, ... ,0)$. This is the **origin** (zero-vector, null-vector). It's not hard to see that these are the coordinates of the zero vector no matter which basis you choose for the vector space.\n",
    "\n",
    "# Subspace\n",
    "A subspace is a vector space that is contained within another vector space. So every subspace is a vector space in its own right, but it is also defined relative to some other (larger) vector space.\n",
    "\n",
    "A subspace of $R^n$ is a subset $V$ of $R^n$, satisfying:\n",
    "1. **Non-emptiness**: The zero vector is in $V$.\n",
    "2. **Closure under addition**: If $u, v \\in V$, then $u+v \\in V$\n",
    "2. **Closure under scalar multiplication**: If $v \\in V$ and $c \\in R$, then $cv \\in V$\n",
    "\n",
    "**Example**: The set $\\R^n$ is a subspace of itself: indeed, it contains zero, and is closed under addition and scalar multiplication.\n",
    "\n",
    "**Example**: The set ${0}$ containing only the zero vector is a subspace of $\\R^n$: it contains zero, and if you add zero to itself or multiply it by a scalar, you always get zero.\n",
    "\n",
    "**Example**: Every line $L$ through the *origin* is a subspace. Indeed, $L$ contains zero, and is easily seen to be closed under addition and scalar multiplication.\n",
    "\n",
    "**Example**: A plane $P$ through the origin is a subspace. Indeed, $P$ contains zero; the sum of two vectors in $P$ is also in $P$; and any scalar multiple of a vector in $P$ is also in $P$.\n",
    "\n",
    "**Non-example**: A line not containing the origin. It fails the first defining property: *every subspace contains the origin by definition.*\n",
    "\n",
    "**Non-example**: A circle. The unit circle is not a subspace. It fails all three defining properties: it does not contain the origin, it is not closed under addition, and it is not closed under scalar multiplication.\n",
    "\n",
    "**Non-example**: A line union a plane. The union of a line and a plane in $R^3$ is not a subspace. It contains the origin and is closed under scalar multiplication, but it is not closed under addition: the sum of a vector on the line and a vector on the plane is not contained in the line or in the plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis\n",
    "A set $B$ of elements of a vector space $V$ is called a **basis** if every element of $V$ can be written in a unique way as a finite **linear combination** of elements of $B$.\n",
    "\n",
    "In other words, **basis** is a subset of the vector space with special properties: it has to *span* the vector space, and it has to be *linearly independent*.\n",
    "\n",
    "A vector space could have many bases.\n",
    "\n",
    "#### Standard Basis\n",
    "The standard basis (also called **natural basis**) is a specific type of basis in which every vector has all **zero** components, except one that equals $1$.\n",
    "\n",
    "$$ e_1 = (1, 0) , e_2 = (0, 1) $$\n",
    "$$ e_1 = (1, 0, 0) , e_2 = (0, 1, 0) , e_3 = (0, 0, 1)$$\n",
    "\n",
    "These vectors are also called **unit vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Combination\n",
    "A **linear combination** or **superposition** is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of $x$ and $y$ would be any expression of the form $ax + by$, where $a$ and $b$ are constants).\n",
    "\n",
    "Let the field $K$ be the set $\\R$ of real numbers, and let the vector space $V$ be $\\R^3$. Consider the vectors $e1 = (1,0,0)$, $e2 = (0,1,0)$ and $e3 = (0,0,1)$. Then any vector in $\\R^3$ is a linear combination of $e1$, $e2$, and $e3$.\n",
    "\n",
    "To see that this is so, take an arbitrary vector $(a_1,a_2,a_3)$ in $\\R^3$, and write:\n",
    "\n",
    "$$ (a_1, a_2, a_3)  $$\n",
    "$$ = (a_1, 0, 0) + (0, a_2, 0) + (0, 0, a_3)$$\n",
    "$$ = a_1(1, 0, 0) + a_2(0, 1, 0) + a_3(0, 0, 1)$$\n",
    "$$ = a_1 e_1 + a_2 e_2 + a_3 e_3$$\n",
    "\n",
    "\n",
    "# Span\n",
    "In linear algebra, the concept of \"span\" is fundamental and helps us understand how sets of vectors can generate entire spaces. The span of a set of vectors is defined as the collection of all possible linear combinations of those vectors. Essentially, if you have a set of vectors, their span includes every vector that can be formed by scaling those vectors and adding them together.\n",
    "\n",
    "For example, if you have two vectors in a two-dimensional space, the span of these vectors can cover the entire plane if the vectors are not collinear. If they are collinear, the span will only cover a line. Similarly, in three dimensions, the span of three vectors can cover the entire space if the vectors are not coplanar.\n",
    "\n",
    "**Definition**: If you have a set of vectors ${v_1, v_2, ..., v_n}$, their span is the collection of vectors that can be expressed in the form:\n",
    "$$c_1 v_1 + c_2 v_2 + ... + c_n v_n$$\n",
    "\n",
    "Where $c_1, c_2, ... , c_n$​ are scalars (real numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Vectors\n",
    "We say that 2 vectors are **orthogonal** if they are perpendicular to each other. i.e. the **dot product of the two vectors is zero**.\n",
    "\n",
    "$$ \\vec{u} = \\begin{pmatrix} 2\\\\ 2 \\end{pmatrix} , \\vec{v} = \\begin{pmatrix} 2\\\\ -2 \\end{pmatrix} $$\n",
    "\n",
    "The dot product of $\\vec{u}$ and $\\vec{v}$ is $(2)(2) + (2)(-2) = 4 - 4 = 0$\n",
    "\n",
    "Thus, $\\vec{u}$ and $\\vec{v}$ are orthogonal.\n",
    "\n",
    "# Orthonormal Vectors\n",
    "A set of vectors $S$ is **orthonormal** if every vector in $S$ has magnitude $1$ and the set of vectors are mutually **orthogonal**.\n",
    "\n",
    "The set of vectors $ \\vec{u} = \\begin{pmatrix} 2\\\\ 2 \\end{pmatrix} , \\vec{v} = \\begin{pmatrix} 2\\\\ -2 \\end{pmatrix} $ is mutually orthogonal, but are not of magnitude 1. Let\n",
    "\n",
    "$$\\vec{u} = \\frac{ \\vec{u}}{|\\vec{u}|} = \\frac{1}{\\sqrt{8}} \\begin{pmatrix} 2\\\\ 2 \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n",
    "\n",
    "\n",
    "$$\\vec{v} = \\frac{ \\vec{u}}{|\\vec{u}|} = \\frac{1}{\\sqrt{8}} \\begin{pmatrix} 2\\\\ -2 \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\frac{1}{\\sqrt{2}}\\\\ - \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n",
    "\n",
    "## Magnitude\n",
    "The formula for calculating **magnitude** $M$ of a $n$-dimensional vector is:\n",
    "$$ M = \\sqrt{a_1^2 + a_2^2 + ... + a_n^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt Process\n",
    "NB: This is an advanced knowledge, not required for this course. It is here only for reference purposes.\n",
    "Given a set of linearly independent vectors, it is often useful to convert them into an orthonormal set of vectors. We first define the projection operator.\n",
    "\n",
    "Let $\\vec{u}$ and $\\vec{v}$ be two vectors. The projection of the vector $\\vec{v}$ on $\\vec{u}$ is defined as folows:\n",
    "$$Proj_{\\vec{u}}\\vec{v} = \\frac{(\\vec{v}.\\vec{u})}{|\\vec{u}|^2}\\vec{u}$$\n",
    "\n",
    "Consider the two vectors $\\vec{v} = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}$ and $\\vec{u} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$. \n",
    "\n",
    "These two vectors are linearly independent. However they are not orthogonal to each other. We create an orthogonal vector in the following manner:\n",
    "\n",
    "$$ \\vec{v_1} = \\vec{v} - (Proj_{\\vec{u}}\\vec{v}) $$\n",
    "\n",
    "$$ Proj_{\\vec{u}}\\vec{v} = \\frac{(1)(1) + (1)(0)}{(\\sqrt{1^2 + 0^2})^2} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} \n",
    "= (1) \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$$\n",
    "\n",
    "$$ \\vec{v_1} = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix} - (1) \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix}$$\n",
    "\n",
    "$\\vec{v_1}$ thus constructed is orthogonal to $\\vec{u}$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Basic principles\n",
    "---\n",
    "1. `n`-dimensional vector space needs **exactly n** vectors to form a basis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "* [Math 2331 – Linear Algebra](https://www.math.uh.edu/~jiwenhe/math2331/lectures/sec4_1.pdf)\n",
    "* [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/subspaces.html)\n",
    "* [Span in Linear Algebra](https://www.geeksforgeeks.org/span-in-linear-algebra/)\n",
    "* [The Nullspace of a Matrix](https://www.cliffsnotes.com/study-guides/algebra/linear-algebra/real-euclidean-vector-spaces/the-nullspace-of-a-matrix)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
