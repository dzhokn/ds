{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "A vector space is a non-empty set $V$ of objects, called vectors, on which are defined two operations, called **addition** and **multiplication** by scalars (real numbers).\n",
    "\n",
    "We can think of a vector space in general, as a collection of objects that behave as vectors do in $\\R^n$. The objects of such a set\n",
    "are called vectors.\n",
    "\n",
    "Vector spaces are subject to next ten axioms for all $u, v$ and $w$ in $V$ and for all scalars $c$ and $d$.\n",
    "1. $u + v \\in V$ \n",
    "2. $u + v = v + u$\n",
    "3. $(u + v) + w = u + (v + w)$\n",
    "4. There is a vector $0 \\in V$ such that $u + 0 = u$\n",
    "5. For $\\forall u$  exists ${-u}$ such that $u + (−u) = 0$\n",
    "6. $cu \\in V$\n",
    "7. $c(u + v) =cu+cv$\n",
    "8. $(c + d)u = cu + du$\n",
    "9. $(cd)u = c(du)$\n",
    "10. There is a vector $1 \\in V$ such that $1u = u$\n",
    "\n",
    "## Vector Space Examples\n",
    "### Field (1-D)\n",
    "A field $F$ is basically 1-dimensional vector space over itself.\n",
    "\n",
    "Vector addition is just field addition, and scalar multiplication is just field multiplication. This property can be used to prove that a field is a vector space. Any non-zero element of $F$ serves as a **basis** (because we can represent all other elements in $F$ by it).\n",
    "\n",
    "### Coordinate Space (n-D)\n",
    "Every coordinate space $\\R^n$ is an $n$-dimensional vector space over the field $\\R$. An element of $\\R^n$ is written:\n",
    "$$u = (u_1, u_2, ..., u_n)$$\n",
    "\n",
    "The vector space $\\R^n$ has a standard basis:\n",
    "$$ e_1 = (1, 0, ..., 0) $$\n",
    "$$ e_2 = (0, 1, ..., 0) $$\n",
    "$$ \\vdots $$\n",
    "$$ e_n = (0, 0, ..., 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Origin\n",
    "Any vector space must have a $0$-vector $(0,0, ... ,0)$. This is the **origin** (zero-vector, null-vector). It's not hard to see that these are the coordinates of the zero vector no matter which basis you choose for the vector space.\n",
    "\n",
    "# Subspace\n",
    "A subspace is a vector space that is contained within another vector space. So every subspace is a vector space in its own right, but it is also defined relative to some other (larger) vector space.\n",
    "\n",
    "A subspace of $R^n$ is a subset $V$ of $R^n$, satisfying:\n",
    "1. **Non-emptiness**: The zero vector is in $V$.\n",
    "2. **Closure under addition**: If $u, v \\in V$, then $u+v \\in V$\n",
    "2. **Closure under scalar multiplication**: If $v \\in V$ and $c \\in R$, then $cv \\in V$\n",
    "\n",
    "**Example**: The set $\\R^n$ is a subspace of itself: indeed, it contains zero, and is closed under addition and scalar multiplication.\n",
    "\n",
    "**Example**: The set ${0}$ containing only the zero vector is a subspace of $\\R^n$: it contains zero, and if you add zero to itself or multiply it by a scalar, you always get zero.\n",
    "\n",
    "**Example**: Every line $L$ through the *origin* is a subspace. Indeed, $L$ contains zero, and is easily seen to be closed under addition and scalar multiplication.\n",
    "\n",
    "**Example**: A plane $P$ through the origin is a subspace. Indeed, $P$ contains zero; the sum of two vectors in $P$ is also in $P$; and any scalar multiple of a vector in $P$ is also in $P$.\n",
    "\n",
    "**Non-example**: A line not containing the origin. It fails the first defining property: *every subspace contains the origin by definition.*\n",
    "\n",
    "**Non-example**: A circle. The unit circle is not a subspace. It fails all three defining properties: it does not contain the origin, it is not closed under addition, and it is not closed under scalar multiplication.\n",
    "\n",
    "**Non-example**: A line union a plane. The union of a line and a plane in $R^3$ is not a subspace. It contains the origin and is closed under scalar multiplication, but it is not closed under addition: the sum of a vector on the line and a vector on the plane is not contained in the line or in the plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis\n",
    "A set $B$ of elements of a vector space $V$ is called a **basis** if every element of $V$ can be written in a unique way as a finite **linear combination** of elements of $B$.\n",
    "\n",
    "In other words, **basis** is a subset of the vector space with special properties: it has to *span* the vector space, and it has to be *linearly independent*.\n",
    "\n",
    "A vector space could have many bases.\n",
    "\n",
    "#### Standard Basis\n",
    "The standard basis (also called **natural basis**) is a specific type of basis in which every vector has all **zero** components, except one that equals $1$.\n",
    "\n",
    "$$ e_1 = (1, 0) , e_2 = (0, 1) $$\n",
    "$$ e_1 = (1, 0, 0) , e_2 = (0, 1, 0) , e_3 = (0, 0, 1)$$\n",
    "\n",
    "These vectors are also called **unit vectors**.\n",
    "\n",
    "---\n",
    "* `n`-dimensional vector space needs **exactly n** vectors to form a basis\n",
    "* any pair of linearly independent vectors forms a basis in 2D space\n",
    "---\n",
    "\n",
    "#### Linearly Independent\n",
    "Two or more vectors are said to be linearly independent if none of them can be written as a **linear combination** of the others.\n",
    "\n",
    "Given a set of vectors, you can determine if they are linearly independent by writing the vectors as the columns of the matrix $A$, and solving $Ax = 0$. If there are any non-zero solutions, then the vectors are linearly dependent. If the only solution is $x = 0$, then they are linearly independent.\n",
    "\n",
    "#### Collinear Vectors\n",
    "Collinear vectors are defined as two or more vectors that are parallel to one another, regardless of their magnitude or direction, and that are parallel to one another.\n",
    "\n",
    "![Collinear vectors](img/collinear_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Combination\n",
    "A **linear combination** or **superposition** is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of $x$ and $y$ would be any expression of the form $ax + by$, where $a$ and $b$ are constants).\n",
    "\n",
    "Let the field $K$ be the set $\\R$ of real numbers, and let the vector space $V$ be $\\R^3$. Consider the vectors $e1 = (1,0,0)$, $e2 = (0,1,0)$ and $e3 = (0,0,1)$. Then any vector in $\\R^3$ is a linear combination of $e1$, $e2$, and $e3$.\n",
    "\n",
    "To see that this is so, take an arbitrary vector $(a_1,a_2,a_3)$ in $\\R^3$, and write:\n",
    "\n",
    "$$ (a_1, a_2, a_3)  $$\n",
    "$$ = (a_1, 0, 0) + (0, a_2, 0) + (0, 0, a_3)$$\n",
    "$$ = a_1(1, 0, 0) + a_2(0, 1, 0) + a_3(0, 0, 1)$$\n",
    "$$ = a_1 e_1 + a_2 e_2 + a_3 e_3$$\n",
    "\n",
    "\n",
    "# Span\n",
    "In linear algebra, the concept of \"span\" is fundamental and helps us understand how sets of vectors can generate entire spaces. The span of a set of vectors is defined as the collection of all possible linear combinations of those vectors. Essentially, if you have a set of vectors, their span includes every vector that can be formed by scaling those vectors and adding them together.\n",
    "\n",
    "For example, if you have two vectors in a two-dimensional space, the span of these vectors can cover the entire plane if the vectors are not collinear. If they are collinear, the span will only cover a line. Similarly, in three dimensions, the span of three vectors can cover the entire space if the vectors are not coplanar.\n",
    "\n",
    "**Definition**: If you have a set of vectors ${v_1, v_2, ..., v_n}$, their span is the collection of vectors that can be expressed in the form:\n",
    "$$c_1 v_1 + c_2 v_2 + ... + c_n v_n$$\n",
    "\n",
    "Where $c_1, c_2, ... , c_n$​ are scalars (real numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Vectors\n",
    "We say that 2 vectors are **orthogonal** if they are perpendicular to each other. i.e. the **dot product of the two vectors is zero**.\n",
    "\n",
    "$$ \\vec{u} = \\begin{pmatrix} 2\\\\ 2 \\end{pmatrix} , \\vec{v} = \\begin{pmatrix} 2\\\\ -2 \\end{pmatrix} $$\n",
    "\n",
    "The dot product of $\\vec{u}$ and $\\vec{v}$ is $(2)(2) + (2)(-2) = 4 - 4 = 0$\n",
    "\n",
    "Thus, $\\vec{u}$ and $\\vec{v}$ are orthogonal.\n",
    "\n",
    "# Orthonormal Vectors\n",
    "A set of vectors $S$ is **orthonormal** if every vector in $S$ has magnitude $1$ and the set of vectors are mutually **orthogonal**.\n",
    "\n",
    "The set of vectors $ \\vec{u} = \\begin{pmatrix} 2\\\\ 2 \\end{pmatrix} , \\vec{v} = \\begin{pmatrix} 2\\\\ -2 \\end{pmatrix} $ is mutually orthogonal, but are not of magnitude 1. Let\n",
    "\n",
    "$$\\vec{u} = \\frac{ \\vec{u}}{|\\vec{u}|} = \\frac{1}{\\sqrt{8}} \\begin{pmatrix} 2\\\\ 2 \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n",
    "\n",
    "\n",
    "$$\\vec{v} = \\frac{ \\vec{u}}{|\\vec{u}|} = \\frac{1}{\\sqrt{8}} \\begin{pmatrix} 2\\\\ -2 \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\frac{1}{\\sqrt{2}}\\\\ - \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n",
    "\n",
    "## Magnitude\n",
    "The formula for calculating **magnitude** $M$ of a $n$-dimensional vector is:\n",
    "$$ M = \\sqrt{a_1^2 + a_2^2 + ... + a_n^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt Process\n",
    "NB: This is an advanced knowledge, not required for this course. It is here only for reference purposes.\n",
    "Given a set of linearly independent vectors, it is often useful to convert them into an orthonormal set of vectors. We first define the projection operator.\n",
    "\n",
    "Let $\\vec{u}$ and $\\vec{v}$ be two vectors. The projection of the vector $\\vec{v}$ on $\\vec{u}$ is defined as folows:\n",
    "$$Proj_{\\vec{u}}\\vec{v} = \\frac{(\\vec{v}.\\vec{u})}{|\\vec{u}|^2}\\vec{u}$$\n",
    "\n",
    "Consider the two vectors $\\vec{v} = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}$ and $\\vec{u} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$. \n",
    "\n",
    "These two vectors are linearly independent. However they are not orthogonal to each other. We create an orthogonal vector in the following manner:\n",
    "\n",
    "$$ \\vec{v_1} = \\vec{v} - (Proj_{\\vec{u}}\\vec{v}) $$\n",
    "\n",
    "$$ Proj_{\\vec{u}}\\vec{v} = \\frac{(1)(1) + (1)(0)}{(\\sqrt{1^2 + 0^2})^2} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} \n",
    "= (1) \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$$\n",
    "\n",
    "$$ \\vec{v_1} = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix} - (1) \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix}$$\n",
    "\n",
    "$\\vec{v_1}$ thus constructed is orthogonal to $\\vec{u}$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "* [Math 2331 – Linear Algebra](https://www.math.uh.edu/~jiwenhe/math2331/lectures/sec4_1.pdf)\n",
    "* [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/subspaces.html)\n",
    "* [Span in Linear Algebra](https://www.geeksforgeeks.org/span-in-linear-algebra/)\n",
    "* [The Nullspace of a Matrix](https://www.cliffsnotes.com/study-guides/algebra/linear-algebra/real-euclidean-vector-spaces/the-nullspace-of-a-matrix)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
